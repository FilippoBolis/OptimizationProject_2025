{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimization - Exercise 2 - Gradient Descent**\n",
        "---"
      ],
      "metadata": {
        "id": "LVxFcj4x-bj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zfVlHN4t2sOm"
      },
      "outputs": [],
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the minimum of a function\n",
        "\n",
        "We start with a very simple implementation of the Gradient Descent method to illustrate the underlying concept.\n",
        "\n",
        "Consider the function $f(x) = x^2$, which is convex and has a unique global minimum $x^*$ (the minimum is obviously $x^* = 0$, but let's pretend we don't know it)."
      ],
      "metadata": {
        "id": "s7SEFeVHWKKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(x):\n",
        "    return x**2\n",
        "\n",
        "# sample input range [-1,1] uniformly at 0.1 increments\n",
        "inputs = np.arange(-1, 1+0.1, 0.1)\n",
        "\n",
        "results = objective(inputs)\n",
        "plt.plot(inputs, results)"
      ],
      "metadata": {
        "id": "8nH5Q_w_aOah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find $x^*$ such that $f(x^*)$ is minimum, we start from an arbitrary initial value $x_0 \\in dom(f)$, we choose a learning rate $\\gamma$, and iterate the step $x = x - \\gamma \\nabla f(x)$.\n",
        "\n",
        "Since we have just one variable, the gradient of $f(x)$ is its derivative: $\\nabla f(x) = \\nabla (x^2) = 2x$, so the gradient descent step takes the form $x = x - \\gamma 2 x$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ftBKT0vMamtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(x):\n",
        "    return 2*x\n",
        "\n",
        "def naive_gradient_descent(x_0, max_iters, gamma):\n",
        "    solutions = [x_0]\n",
        "    scores = [objective(x_0)]\n",
        "    x = x_0\n",
        "\n",
        "    # Simple implementation with a fixed number of iterations\n",
        "    for i in range(max_iters):\n",
        "        gradient = derivative(x)\n",
        "        x = x - gamma * gradient\n",
        "        score = objective(x)\n",
        "\n",
        "        solutions.append(x)\n",
        "        scores.append(score)\n",
        "    return solutions, scores"
      ],
      "metadata": {
        "id": "CvuLhQBbb9ex"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test this implementation trying various values for $x_0$ and $\\gamma$. Note that the naive Gradient Descent method is **not** guaranteed to converge for arbitrary values of $\\gamma$. In this case, it is quite easy to see that $x = x - \\gamma 2 x$ converges to $0$ only for $\\gamma < 1$."
      ],
      "metadata": {
        "id": "-HhsEsdYkAjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values for x_0 and gamma\n",
        "x_0 = 0.8\n",
        "max_iters = 30\n",
        "gamma = 0.1\n",
        "\n",
        "solutions, scores = naive_gradient_descent(x_0, max_iters, gamma)\n",
        "\n",
        "# plot f(x) again\n",
        "plt.plot(inputs, results)\n",
        "\n",
        "#plot the values of x at the various steps (in red)\n",
        "plt.plot(solutions, scores, '.-', color='red')\n",
        "\n",
        "# read the best value of x found so far\n",
        "print(\"best value of x after\", max_iters, \"iterations:\" , solutions[-1])"
      ],
      "metadata": {
        "id": "Z3pt4SvGcbeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fFLhTY02sOv"
      },
      "source": [
        "# Least Squares Estimation\n",
        "Least squares estimation is one of the fundamental machine learning algorithms. Suppose we want to fit a hyperplane to a set of data points $x_1, \\dots , x_n \\in \\mathbb{R}^d$, based on the hypothesis that the points actually come (approximately) from a hyperplane. For simplicity, let us do this in $\\mathbb{R}^2$ and fit a linear model, searching for a non-vertical line of the form $y = w_0 + w_1 x$.\n",
        "\n",
        "![](https://i.ibb.co/7k78YDd/least-square.png)\n",
        "\n",
        "If we have $n$ data points from $(x_1, y_1)$ to $(x_n, y_n)$, the least square fit chooses $w_0$ and $w_1$ such that $f(w_0, w_1) = \\sum_{i=1}^n(w_0 + w_1x_i - y_i)^2$ is minimized. <br />\n",
        "This sum can be expressed in matrix form as $\\|Ax - b\\|^2$, where:\n",
        "<br /><br />\n",
        "$Ax - b =\n",
        "     \\begin{bmatrix}\n",
        "         1 & x_1\\\\\n",
        "         1 & x_2\\\\\n",
        "         \\vdots & \\vdots\\\\\n",
        "         1 & x_n\n",
        "     \\end{bmatrix}\n",
        "     \\times\n",
        "     \\begin{bmatrix}\n",
        "         w_0\\\\\n",
        "         w_1\\\\\n",
        "     \\end{bmatrix}\n",
        "      -\n",
        "     \\begin{bmatrix}\n",
        "         y_1\\\\\n",
        "         y_2\\\\\n",
        "         \\vdots\\\\\n",
        "         y_n\n",
        "     \\end{bmatrix}\n",
        "     =\n",
        "     \\begin{bmatrix}\n",
        "         w_0 + x_1w_1 - y_1\\\\\n",
        "         w_0 + x_2w_1 - y_2\\\\\n",
        "         \\vdots\\\\\n",
        "         w_0 + x_nw_1 - y_n\\\\\n",
        "     \\end{bmatrix}\n",
        "$\n",
        "<br /><br />\n",
        "In this exercise, we will try to find $x = [w_0, w_1]^\\top$ using Least Squares Estimation, implementing the Gradient Descent method.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsuOelqo2sOs"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "For this first implementation, we will load the [breast cancer dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset), using the first and third attributes (tumor mean radius and perimeter) as datapoints in two dimensions and trying to fit them on a line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7zXbpaWp2sOt"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "def standardize(x):\n",
        "    \"\"\"Standardize the original data points (mean 0 and std dev 1).\"\"\"\n",
        "    x = x - np.mean(x)\n",
        "    x = x / np.std(x)\n",
        "    return x\n",
        "\n",
        "def build_model_data(x, y):\n",
        "    \"\"\"Get regression data in matrix form.\"\"\"\n",
        "    b = y\n",
        "    num_samples = len(b)\n",
        "    A = np.c_[np.ones(num_samples), x]\n",
        "    return A, b\n",
        "\n",
        "# Loads sklearn's Breast Cancer dataset\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# Use as datapoints the first and third columns\n",
        "radius = dataset.data[:,0]\n",
        "perimeter = dataset.data[:,2]\n",
        "\n",
        "A, b = build_model_data(standardize(radius), standardize(perimeter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR5BWSrJ2sOu"
      },
      "outputs": [],
      "source": [
        "# Take a look at our data\n",
        "plt.xlabel ('radius')\n",
        "plt.ylabel ('perimeter')\n",
        "plt.scatter(A[:,1], b)\n",
        "\n",
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK7cFxO42sOz"
      },
      "source": [
        "# Gradient Descent Implementation\n",
        "\n",
        "Complete the `gradient_descent` function below, filling in also the `compute_gradient` and `calculate_objective` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8jvdSBHC2sO0"
      },
      "outputs": [],
      "source": [
        "def calculate_objective(Axmb):\n",
        "    \"\"\"Calculate ||Ax - b||^2 for the vector Axmb = Ax - b\"\"\"\n",
        "    # ***************************************************\n",
        "    # YOUR CODE HERE\n",
        "    obj = (Axmb**2).sum()\n",
        "    #obj = (Axmb).T.dot(Axmb)\n",
        "    # ***************************************************\n",
        "    return obj\n",
        "\n",
        "def compute_gradient(A, x, b):\n",
        "    \"\"\"Compute the gradient and objective function.\"\"\"\n",
        "    # ***************************************************\n",
        "    # YOUR CODE HERE\n",
        "    Axmb = A.dot(x) - b\n",
        "    grad = 2 * A.T.dot(Axmb)\n",
        "    # ***************************************************\n",
        "    return grad, Axmb\n",
        "\n",
        "def gradient_descent(A, initial_x, b, max_iters, gamma):\n",
        "    \"\"\"Gradient descent algorithm.\"\"\"\n",
        "    # Define parameters to store x and objective func. values\n",
        "    xs = [initial_x]\n",
        "    objectives = []\n",
        "    x = initial_x\n",
        "    for n_iter in range(max_iters):\n",
        "\n",
        "        # compute objective and gradient\n",
        "        grad, Axmb = compute_gradient(A, x, b)\n",
        "        obj = calculate_objective(Axmb)\n",
        "\n",
        "        # ***************************************************\n",
        "        # YOUR CODE HERE\n",
        "        # update x by a gradient descent step\n",
        "        x = x - gamma * grad\n",
        "        # ***************************************************\n",
        "\n",
        "        # store x and objective function value\n",
        "        xs.append(x)\n",
        "        objectives.append(obj)\n",
        "        print(\"Gradient Descent({bi}/{ti}): objective={l:.5f}, x=[{w0:.5f},{w1:.5f}]\".format(\n",
        "              bi=n_iter, ti=max_iters - 1, l=obj, w0=x[0], w1=x[1]))\n",
        "\n",
        "    return objectives, xs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmpCIg1U2sO1"
      },
      "source": [
        "## Naive test\n",
        "\n",
        "Now you can test your gradient descent function with a naive step size through gradient descent demo shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlRtIH8W2sO1"
      },
      "outputs": [],
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = 30\n",
        "gamma = 0.001  # gamma = 0.1 does not converge\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "gradient_objectives_naive, gradient_xs_naive = gradient_descent(A, x_initial, b, max_iters, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w0_star = gradient_xs_naive[-1][0]\n",
        "w1_star = gradient_xs_naive[-1][1]\n",
        "\n",
        "# Visualize our solution\n",
        "plt.xlabel ('radius')\n",
        "plt.ylabel ('perimeter')\n",
        "plt.scatter(A[:,1], b)\n",
        "\n",
        "plt.axline((0, w0_star), slope=w1_star, color='red')"
      ],
      "metadata": {
        "id": "c4ldekQzD6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W16cyZFp2sO2"
      },
      "source": [
        "Now try doing gradient descent with a better learning rate instead of a fixed $\\gamma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7gTu3hS2sOx"
      },
      "source": [
        "# Gradient descent with smoothness constant $L$\n",
        "\n",
        "In general, quadratic functions are $L$-smooth. It can be proved that function $f = \\|Ax - b\\|^2$ is $L$-smooth with $L = 2\\|A^\\top A\\|$. Please fill in the function `calculate_L` below and use it to improve the learning rate of the algorithm.\n",
        "\n",
        "*Hint*: you can use the function `np.linalg.norm` with parameter `ord=2` to compute the spectral norm of a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jPqXRikH2sOy"
      },
      "outputs": [],
      "source": [
        "def calculate_L(A, b):\n",
        "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
        "    # ***************************************************\n",
        "    # YOUR CODE HERE\n",
        "    # compute L = smoothness constant of f\n",
        "    L = 2 * np.linalg.norm(A.T.dot(A), ord=2)\n",
        "    # ***************************************************\n",
        "    return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZZ-S1mg2sO2"
      },
      "outputs": [],
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = 30\n",
        "\n",
        "# ***************************************************\n",
        "# YOUR CODE HERE\n",
        "# a better learning rate using the smoothness of f\n",
        "L = calculate_L(A, b)\n",
        "gamma = 1/L\n",
        "# ***************************************************\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "gradient_objectives, gradient_xs = gradient_descent(A, x_initial, b, max_iters, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w0_star = gradient_xs[-1][0]\n",
        "w1_star = gradient_xs[-1][1]\n",
        "\n",
        "# Visualize our solution\n",
        "plt.xlabel ('radius')\n",
        "plt.ylabel ('perimeter')\n",
        "plt.scatter(A[:,1], b)\n",
        "\n",
        "plt.axline((0, w0_star), slope=w1_star, color='red')"
      ],
      "metadata": {
        "id": "b9jGmzINhU_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eE5YQ5g2sO3"
      },
      "source": [
        "# Loading more complex data\n",
        "\n",
        "The same gradient descent algorithm can be applied in any number $d$ of dimensions (corresponding to columns of the matrix $A$). In this case we are not searching for a line anymore.\n",
        "\n",
        "With $d$ dimensions we have variable weights $w_0 , \\mathbf{w} = (w_1, \\dots, w_d) \\in \\mathbb{R}^d$ , and our aim is to minimize the least squares objective $f(w_0, \\dots, w_n) = \\sum_{i=1}^n(w_0 + \\mathbf{w}^{\\top}x_i - y_i)^2$.\n",
        "\n",
        "This time we will load the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset), using all the eight attributes as $x_i$ and the label (house value) as $y_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h8zC2rIm2sO3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Loads sklearn's California Housing dataset\n",
        "dataset = fetch_california_housing()\n",
        "\n",
        "# Use all but the last column as dimensions\n",
        "features = dataset.data\n",
        "prices = dataset.target\n",
        "\n",
        "A, b = build_model_data(standardize(features), standardize(prices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N6sFgZvs2sO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e97fff7-6bc2-447d-edb5-db65fdbbb931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples n =  20640\n",
            "Dimension of each sample d =  9\n"
          ]
        }
      ],
      "source": [
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t74gqx552sO4"
      },
      "source": [
        "# Running gradient descent\n",
        "\n",
        "We start by running the previously implemented versions of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 50\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "gamma_naive = 0.000001 # gamma = 0.00001 does not converge\n",
        "gamma_smooth = 1/calculate_L(A,b)\n",
        "\n",
        "# Naive version\n",
        "gradient_objectives_naive, gradient_xs_naive = gradient_descent(A, x_initial, b, max_iters, gamma_naive)\n",
        "print(\"########################\")\n",
        "\n",
        "# Smoothness version\n",
        "gradient_objectives_smooth, gradient_xs_smooth = gradient_descent(A, x_initial, b, max_iters, gamma_smooth)\n",
        "\n",
        "# Print the estimated coefficients\n",
        "print(gradient_xs_naive[-1])\n",
        "print(gradient_xs_smooth[-1])"
      ],
      "metadata": {
        "id": "BLYgRtuxxenJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EizWrOI2sO4"
      },
      "source": [
        "## Assuming bounded gradients (Lipschitz function)\n",
        "Assume we are moving in a bounded region $\\|x\\| \\leq R$ containing all iterates (and we assume $\\|x_0-x^\\star\\| \\leq R$ as well, for simplicity).\n",
        "\n",
        "Then by $\\nabla f(x) = 2 A^\\top (Ax - b)$, it can be proven that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq 2 (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$.\n",
        "\n",
        "Please fill in the learning rate assuming bounded gradients with $R=25$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMOaDAlU2sO5"
      },
      "outputs": [],
      "source": [
        "max_iters = 50\n",
        "R = 25\n",
        "\n",
        "# ***************************************************\n",
        "# YOUR CODE HERE\n",
        "# Compute the bound B on the gradient norm\n",
        "B = 2 * (R*np.linalg.norm(np.dot(A.T,A)) + np.linalg.norm(np.dot(A.T,b)))\n",
        "# ***************************************************\n",
        "\n",
        "# ***************************************************\n",
        "# YOUR CODE HERE\n",
        "#  Compute learning rate based on bounded gradient\n",
        "gamma_bounded = R/(B*np.sqrt(max_iters))\n",
        "# ***************************************************\n",
        "\n",
        "# Start gradient descent.\n",
        "gradient_objectives_bounded, gradient_xs_bounded = gradient_descent(A, x_initial, b, max_iters, gamma_bounded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIUdLhON2sO6"
      },
      "source": [
        "## Plotting the Evolution of the Objective Function\n",
        "\n",
        "Since we have data points in multiple dimensions, in order to visualize the results we compare the evolution of the objective function with respect to the number of steps. The algorithm with the better learning rate should converge faster to the minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iRTlwjN2sO6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.xlabel('Number of steps')\n",
        "plt.ylabel('Objective Function')\n",
        "\n",
        "plt.plot(range(len(gradient_objectives_naive)), gradient_objectives_naive,'red', label='naive gradient descent')\n",
        "plt.plot(range(len(gradient_objectives_bounded)), gradient_objectives_bounded,'blue', label='gradient descent assuming bounded gradients')\n",
        "plt.plot(range(len(gradient_objectives_smooth)), gradient_objectives_smooth,'green', label='gradient descent assuming smoothness')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}